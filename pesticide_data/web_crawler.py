import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

all_data = []

# å®šç¾©éœ€è¦çš„æ¬„ä½
target_columns = [
    "è¨±å¯è­‰è™Ÿç¢¼", "è¨±å¯è­‰è™Ÿç¢¼_é€£çµ",
    "æ™®é€šåç¨±", "æ™®é€šåç¨±_é€£çµ",
    "å» ç‰Œåç¨±",
    "åŠ‘å‹",
    "å«é‡",
    "Under Permit",
    "æ··åˆ",
    "å» å•†åç¨±", "å» å•†åç¨±_é€£çµ",
    "åœ‹å¤–åŸè£½é€ å» ", "åœ‹å¤–åŸè£½é€ å» _é€£çµ",
    "æœ‰æ•ˆæ—¥æœŸ",
    "æ¨™ç¤º_é€£çµ",
    "æ¢ç¢¼_é€£çµ",
    "ä½¿ç”¨ç¯„åœ_é€£çµ",
]

for page in range(1, 65):
    print(f"ğŸ“„ æ­£åœ¨æŠ“ç¬¬ {page} é ")
    url = f"https://pesticide.aphia.gov.tw/information/Query/RegisterList/?type=1&page={page}&pagesize=100"
    headers = {"User-Agent": "Mozilla/5.0"}

    response = requests.get(url, headers=headers)
    response.encoding = "utf-8"

    soup = BeautifulSoup(response.text, "html.parser")
    rows = soup.select("div.table-data-list tbody tr")

    for row in rows:
        tds = row.find_all("td")
        if len(tds) < 10:  # åŸºæœ¬æ¬„ä½æª¢æŸ¥
            print(f"â›” æ¬„ä½ä¸è¶³ (åªæœ‰ {len(tds)} å€‹)ï¼Œè·³é")
            continue

        # å–å¾—åŸå§‹æ¬„ä½æ–‡å­—èˆ‡è¶…é€£çµ
        def get_text_and_link(td):
            text = td.get_text(strip=True)  # å–å¾—ç´”æ–‡å­—ä¸¦å»é™¤ç©ºç™½
            a_tag = td.find("a")  # æª¢æŸ¥æ˜¯å¦æœ‰è¶…é€£çµ(åŒ…åœ¨å…§)
            href = a_tag["href"] if a_tag and a_tag.get("href") else ""
            full_url = f"https://pesticide.aphia.gov.tw{href}" if href.startswith("/") else ""
            return text, full_url

        # è™•ç†è¨±å¯è­‰è™Ÿç¢¼ (æ‹†åˆ†ç‚ºé¡å‹å’Œè™Ÿç¢¼)
        permit_text, permit_link = get_text_and_link(tds[0])
        permit_parts = permit_text.strip().split()
        if len(permit_parts) >= 2:
            permit_type = permit_parts[0]
            permit_number = permit_parts[-1]
        else:
            permit_type = permit_text
            permit_number = ""

        row_data = {
            "è¨±å¯è­‰é¡å‹": permit_type,
            "è¨±å¯è­‰è™Ÿç¢¼": permit_number,
            "è¨±å¯è­‰è™Ÿç¢¼_é€£çµ": permit_link,
            "æ™®é€šåç¨±": get_text_and_link(tds[1])[0],
            "æ™®é€šåç¨±_é€£çµ": get_text_and_link(tds[1])[1],
            "å» ç‰Œåç¨±": get_text_and_link(tds[2])[0],
            "åŠ‘å‹": get_text_and_link(tds[3])[0],
            "å«é‡": get_text_and_link(tds[4])[0],
            "Under Permit": get_text_and_link(tds[5])[0],
            "æ··åˆ": get_text_and_link(tds[6])[0],
            "å» å•†åç¨±": get_text_and_link(tds[7])[0],
            "å» å•†åç¨±_é€£çµ": get_text_and_link(tds[7])[1],
            "åœ‹å¤–åŸè£½é€ å» ": get_text_and_link(tds[8])[0],
            "åœ‹å¤–åŸè£½é€ å» _é€£çµ": get_text_and_link(tds[8])[1],
            "æœ‰æ•ˆæ—¥æœŸ": get_text_and_link(tds[9])[0],
            "æ¨™ç¤º_é€£çµ": get_text_and_link(tds[11])[1] if len(tds) > 11 else "",
            "æ¢ç¢¼_é€£çµ": get_text_and_link(tds[12])[1] if len(tds) > 12 else "",
            "ä½¿ç”¨ç¯„åœ_é€£çµ": get_text_and_link(tds[13])[1] if len(tds) > 13 else "",
        }

        all_data.append(row_data)
        print("âœ… åŠ å…¥ä¸€ç­†è³‡æ–™")
    time.sleep(0.5)

# å»ºç«‹DataFrameä¸¦åªä¿ç•™éœ€è¦çš„æ¬„ä½
final_columns = [
    "è¨±å¯è­‰é¡å‹", "è¨±å¯è­‰è™Ÿç¢¼", "è¨±å¯è­‰è™Ÿç¢¼_é€£çµ",
    "æ™®é€šåç¨±", "æ™®é€šåç¨±_é€£çµ",
    "å» ç‰Œåç¨±",
    "åŠ‘å‹",
    "å«é‡",
    "Under Permit",
    "æ··åˆ",
    "å» å•†åç¨±", "å» å•†åç¨±_é€£çµ",
    "åœ‹å¤–åŸè£½é€ å» ", "åœ‹å¤–åŸè£½é€ å» _é€£çµ",
    "æœ‰æ•ˆæ—¥æœŸ",
    "æ¨™ç¤º_é€£çµ",
    "æ¢ç¢¼_é€£çµ",
    "ä½¿ç”¨ç¯„åœ_é€£çµ",
]

df = pd.DataFrame(all_data)[final_columns]
df.to_csv("taiwan_pesticides_cleaned.csv", index=False, encoding="utf-8-sig")
print(f"âœ… å®Œæˆï¼Œå…±æŠ“åˆ° {len(df)} ç­†è³‡æ–™ï¼Œæ¬„ä½å·²æ¸…ç†ä¸¦å‘½å")