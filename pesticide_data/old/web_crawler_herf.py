import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

all_data = []

for page in range(1, 65):
    print(f"ğŸ“„ æ­£åœ¨æŠ“ç¬¬ {page} é ")
    url = f"https://pesticide.aphia.gov.tw/information/Query/RegisterList/?type=1&page={page}&pagesize=100"

    headers = {
        "User-Agent": "Mozilla/5.0",
    }

    response = requests.get(url, headers=headers)
    response.encoding = 'utf-8'

    soup = BeautifulSoup(response.text, "html.parser")
    rows = soup.select("div.table-data-list tbody tr")

    for row in rows:
        cols = []
        for td in row.find_all("td"):
            text = td.get_text(strip=True)
            link = td.find("a")
            href = link["href"] if link and link.get("href") else ""
            full_url = f"https://pesticide.aphia.gov.tw{href}" if href.startswith("/") else href
            cols.extend([text, full_url])
        all_data.append(cols)

    time.sleep(0.5)

# åŠ å…¥æ¬„ä½åç¨±
columns = [
    "è¨±å¯è­‰è™Ÿç¢¼", "è¨±å¯è­‰è™Ÿç¢¼_é€£çµ",
    "æ™®é€šåç¨±", "æ™®é€šåç¨±_é€£çµ",
    "å» ç‰Œåç¨±", "å» ç‰Œåç¨±_é€£çµ",
    "åŠ‘å‹", "åŠ‘å‹_é€£çµ",
    "å«é‡", "å«é‡_é€£çµ",
    "Under Permit", "Under Permit_é€£çµ",
    "æ··åˆ", "æ··åˆ_é€£çµ",
    "å» å•†åç¨±", "å» å•†åç¨±_é€£çµ",
    "åœ‹å¤–åŸè£½é€ å» ", "åœ‹å¤–åŸè£½é€ å» _é€£çµ",
    "æœ‰æ•ˆæ—¥æœŸ", "æœ‰æ•ˆæ—¥æœŸ_é€£çµ",
    "å‚™è¨»", "å‚™è¨»_é€£çµ",
    "æ¨™ç¤º", "æ¨™ç¤º_é€£çµ",
    "æ¢ç¢¼", "æ¢ç¢¼_é€£çµ",
    "ä½¿ç”¨ç¯„åœ", "ä½¿ç”¨ç¯„åœ_é€£çµ",
]

df = pd.DataFrame(all_data, columns=columns)
df.to_csv("taiwan_pesticides_with_links_named.csv", index=False, encoding="utf-8-sig")
print(f"âœ… å®Œæˆï¼Œå…±æŠ“åˆ° {len(df)} ç­†è³‡æ–™ï¼Œæ¬„ä½å·²å‘½å")
